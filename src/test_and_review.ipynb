{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import torch\n",
    "import pylab as plt\n",
    "# %matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "import memory as mem   \n",
    "from feedforward import Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test in env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "env_name = 'Pendulum-v1'\n",
    "# env_name = 'CartPole-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "if isinstance(env.action_space, spaces.Box):\n",
    "    env = DiscreteActionWrapper(env,5)\n",
    "\n",
    "ac_space = env.action_space\n",
    "o_space = env.observation_space\n",
    "print(ac_space)\n",
    "print(o_space)\n",
    "print(list(zip(env.observation_space.low, env.observation_space.high)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "use_target = True\n",
    "target_update = 20\n",
    "q_agent = DQNAgent(o_space, ac_space, discount=0.95, eps=0.2, \n",
    "                   use_target_net=use_target, update_target_every= target_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ob,_info = env.reset()\n",
    "q_agent.Q.predict(ob)\n",
    "\n",
    "\n",
    "stats = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "max_episodes=600\n",
    "max_steps=500 \n",
    "for i in range(max_episodes):\n",
    "    # print(\"Starting a new episode\")    \n",
    "    total_reward = 0\n",
    "    ob, _info = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        done = False        \n",
    "        a = q_agent.act(ob)\n",
    "        (ob_new, reward, done, trunc, _info) = env.step(a)\n",
    "        total_reward+= reward\n",
    "        q_agent.store_transition((ob, a, reward, ob_new, done))            \n",
    "        ob=ob_new        \n",
    "        if done: break    \n",
    "    losses.extend(q_agent.train(32))\n",
    "    stats.append([i,total_reward,t+1])    \n",
    "    \n",
    "    if ((i-1)%20==0):\n",
    "        print(\"{}: Done after {} steps. Reward: {}\".format(i, t+1, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats_np = np.asarray(stats)\n",
    "losses_np = np.asarray(losses)\n",
    "fig=plt.figure(figsize=(6,3.8))\n",
    "plt.plot(stats_np[:,1], label=\"return\")\n",
    "plt.plot(running_mean(stats_np[:,1],20), label=\"smoothed-return\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fig.savefig(f\"DQN_{env_name}_training_w_target_{use_target}-update-{target_update}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "q_agent.buffer.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "env_eval = gym.make(env_name, render_mode=\"human\")\n",
    "if isinstance(env.action_space, spaces.Box):\n",
    "    env_eval = DiscreteActionWrapper(env_eval,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_stats = []\n",
    "episodes=50\n",
    "env_ = env    # without rendering\n",
    "#env_ = env_eval # with rendering\n",
    "\n",
    "for i in range(episodes):\n",
    "    total_reward = 0\n",
    "    ob, _info = env_.reset()\n",
    "    for t in range(max_steps):\n",
    "        done = False        \n",
    "        a = q_agent.act(ob, eps=0.0)\n",
    "        (ob_new, reward, done, trunc, _info) = env_.step(a)\n",
    "        total_reward+= reward\n",
    "        ob=ob_new        \n",
    "        if done: break    \n",
    "    test_stats.append([i,total_reward,t+1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_stats_np = np.array(test_stats)\n",
    "print(np.mean(test_stats_np[:,1]), \"+-\", np.std(test_stats_np[:,1]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
